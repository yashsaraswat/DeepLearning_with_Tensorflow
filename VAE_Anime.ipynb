{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kq_ruGkVDE5I"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gn2eftMADOXT"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "LATENT_DIM = 256\n",
        "EPOCHS = 100\n",
        "IMG_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFHKdZd5Gsiv",
        "outputId": "e7876df9-18e3-4ffd-f857-72faa978ca91"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('animefaces.zip', <http.client.HTTPMessage at 0x7f9c5a0ef250>)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make the data directory\n",
        "try:\n",
        "  os.mkdir('/tmp/anime')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "# download the zipped dataset to the data directory\n",
        "data_url = \"https://storage.googleapis.com/learning-datasets/Resources/anime-faces.zip\"\n",
        "data_file_name = \"animefaces.zip\"\n",
        "urllib.request.urlretrieve(data_url, data_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k1suBVA5JyWG"
      },
      "outputs": [],
      "source": [
        "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
        "zip_ref.extractall(\"anime\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3_7gRpBjG4_s"
      },
      "outputs": [],
      "source": [
        "def get_dataset_slice_paths(image_dir):\n",
        "  '''returns a list of paths to the image files'''\n",
        "  image_file_list = os.listdir(image_dir)\n",
        "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
        "\n",
        "  return image_paths\n",
        "\n",
        "def map_image(image_filename):\n",
        "  image_raw = tf.io.read_file(image_filename)\n",
        "  image = tf.image.decode_jpeg(image_raw)\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "  image = image/255.0\n",
        "  image = tf.reshape(image, (IMG_SIZE, IMG_SIZE, 3))\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Tp7W7TaPJklh"
      },
      "outputs": [],
      "source": [
        "paths = get_dataset_slice_paths(\"/tmp/anime/images/\")\n",
        "random.shuffle(paths)\n",
        "\n",
        "path_len = len(paths)\n",
        "\n",
        "train_len = int(0.8*path_len)\n",
        "train_path = paths[:train_len+1]\n",
        "validation_path= paths[train_len+1:]\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_path))\n",
        "train_dataset = train_dataset.map(map_image)\n",
        "train_dataset = train_dataset.shuffle(4096).batch(BATCH_SIZE)\n",
        "\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_path))\n",
        "validation_dataset = validation_dataset.map(map_image)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQDck1udLJzC",
        "outputId": "834c6112-ec7a-4d67-fc8a-3f17608a9bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No. of batches in training set: 199\n",
            "No. of batches in validation set: 50\n"
          ]
        }
      ],
      "source": [
        "print(f'No. of batches in training set: {len(train_dataset)}')\n",
        "print(f'No. of batches in validation set: {len(validation_dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgeTqgfxNH2B"
      },
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PEAI3ymXM6gI"
      },
      "outputs": [],
      "source": [
        "class Reparametrize(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    mu, sigma = inputs\n",
        "    batch = tf.shape(sigma)[0]\n",
        "    dim = tf.shape(sigma)[1]\n",
        "    eps = tf.keras.backend.random_normal(shape = (batch, dim))\n",
        "    z = eps * tf.exp(0.5*sigma) + mu\n",
        "\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OeZ-4eN5N2Od"
      },
      "outputs": [],
      "source": [
        "def encoder_layers(inputs, latent_dim):\n",
        "  conv1 = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, strides = 2,\n",
        "                                 activation = \"relu\", padding = \"same\")(inputs)\n",
        "  bn1 = tf.keras.layers.BatchNormalization()(conv1)\n",
        "\n",
        "  conv2 = tf.keras.layers.Conv2D(filters = 128, kernel_size = 3, strides = 2,\n",
        "                                 activation = \"relu\", padding = \"same\")(bn1)\n",
        "  bn2 = tf.keras.layers.BatchNormalization()(conv2)\n",
        "\n",
        "  flat = tf.keras.layers.Flatten()(bn2)\n",
        "  d1 = tf.keras.layers.Dense(20, activation = \"relu\")(flat)\n",
        "  bn3 = tf.keras.layers.BatchNormalization()(d1)\n",
        "\n",
        "  mu = tf.keras.layers.Dense(latent_dim, activation = \"relu\")(bn3)\n",
        "  sigma = tf.keras.layers.Dense(latent_dim, activation = \"relu\")(bn3)\n",
        "\n",
        "  return mu, sigma, bn2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "R_q5A2KEO9aM"
      },
      "outputs": [],
      "source": [
        "def encoder_model(input_shape, latent_dim):\n",
        "  inputs = tf.keras.layers.Input(shape = input_shape)\n",
        "  mu, sigma, conv_shape = encoder_layers(inputs, latent_dim)\n",
        "  z = Reparametrize()((mu, sigma))\n",
        "\n",
        "  model = tf.keras.Model(inputs = inputs, outputs = [mu, sigma, z])\n",
        "\n",
        "  return model, conv_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "btV9as7BQCtg"
      },
      "outputs": [],
      "source": [
        "def decoder_layers(conv_shape, inputs):\n",
        "  units = conv_shape[1]*conv_shape[2]*conv_shape[3]\n",
        "\n",
        "  x = tf.keras.layers.Dense(units, activation = \"relu\")(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
        "\n",
        "  conv1 = tf.keras.layers.Conv2DTranspose(filters = 128, kernel_size = 3, strides = 2,\n",
        "                                 activation = \"relu\", padding = \"same\")(x)\n",
        "  bn1 = tf.keras.layers.BatchNormalization()(conv1)\n",
        "\n",
        "  conv2 = tf.keras.layers.Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2,\n",
        "                                 activation = \"relu\", padding = \"same\")(bn1)\n",
        "  bn2 = tf.keras.layers.BatchNormalization()(conv2)\n",
        "\n",
        "  conv3 = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = 3, strides = 1,\n",
        "                                 activation = \"sigmoid\", padding = \"same\")(bn2)\n",
        "\n",
        "  return conv3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1ptUkBlLRExi"
      },
      "outputs": [],
      "source": [
        "def decoder_model(latent_dim, conv_shape):\n",
        "  inputs = tf.keras.layers.Input(shape = (latent_dim,))\n",
        "  outputs = decoder_layers(conv_shape, inputs)\n",
        "  model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "pNCyIp9jR4DO"
      },
      "outputs": [],
      "source": [
        "def kl_divergence_loss(mu, sigma):\n",
        "  kl_loss = -0.5 * tf.reduce_mean(1 + sigma - tf.square(mu) - tf.math.exp(sigma), axis = 1)\n",
        "\n",
        "  return kl_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "a1CyCG5RRo62"
      },
      "outputs": [],
      "source": [
        "def vae_model(encoder, decoder, input_shape, latent_dim):\n",
        "  inputs = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "  mu, sigma, z = encoder(inputs, latent_dim)\n",
        "  outputs = decoder(z)\n",
        "  kl_loss = kl_divergence_loss(mu, sigma)\n",
        "\n",
        "  model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "  model.add_loss(kl_loss)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Jl21FfqzS2X2"
      },
      "outputs": [],
      "source": [
        "def get_models(input_shape, latent_dim):\n",
        "  encoder, conv_shape = encoder_model(input_shape, latent_dim)\n",
        "  decoder = decoder_model(latent_dim, conv_shape)\n",
        "  vae = vae_model(encoder, decoder, input_shape, latent_dim)\n",
        "  \n",
        "  return encoder, decoder, vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "wjvDkPEnTPdX"
      },
      "outputs": [],
      "source": [
        "encoder, decoder, vae = get_models(input_shape = (64, 64, 3), latent_dim = LATENT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "2gFiyV9iTebq"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "-VL8tNOaiMra"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(model, x, optimizer, mse_loss):\n",
        "  with tf.GradientTape() as tape:\n",
        "    pred = model(x)\n",
        "    \n",
        "    loss = mse_loss(x, pred)*12288 # (64x64x3)\n",
        "    #Scaler is multiploed because we want to take mean on batch_size\n",
        "    # mse calculates. se = (x-x_pred)**2, se.shape = (batch_size*height*width*channels,)\n",
        "    # so mse = se/(batch_size*height*width*channels)\n",
        "    # But we want to normalize by batch_size, so multiply back by (height*width*channels)   \n",
        "    loss +=sum(vae.losses)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "qVDy5wrbmAxd"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input):\n",
        "  \"\"\"Helper function to plot our 16 images\n",
        "\n",
        "  Args:\n",
        "\n",
        "  model -- the decoder model\n",
        "  epoch -- current epoch number during training\n",
        "  step -- current step number during training\n",
        "  test_input -- random tensor with shape (16, LATENT_DIM)\n",
        "  \"\"\"\n",
        "  predictions = model.predict(test_input)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      img = predictions[i, :, :, :] * 255\n",
        "      img = img.astype('int32')\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "  #plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "XkztD89nmCrl"
      },
      "outputs": [],
      "source": [
        "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "4ZLD-221XdM2"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  for step, x_batch_train in enumerate(train_dataset):\n",
        "    loss = train_step(vae, x_batch_train, optimizer, mse_loss)\n",
        "\n",
        "    loss_metric(loss)\n",
        "\n",
        "    if step % 100 == 0:\n",
        "      display.clear_output(wait=False)    \n",
        "      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
        "      print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
